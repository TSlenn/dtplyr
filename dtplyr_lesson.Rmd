---
title: "dtplyr_lesson"
author: "Terry Slenn"
date: "October 29, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r packages}
suppressPackageStartupMessages({
  require(tidyverse)
  require(data.table)
  require(dtplyr)  ## devtools::install_github("tidyverse/dtplyr") May need the devtools version.  Still in development
  require(purrr) ## Good mapping functions
  require(microbenchmark)
  })
```

The data for this example is taken from a high school swim team, with Athlete and School names removed.

```{r load_data}
load("dtdata.RData")
head(Athlete)
head(Meet)
head(Results)
```

## Dplyr/tidyverse

The tidyverse packages in R are one of the major reasons for Rs popularity.  It allows for easy to comment code that reads like a sentence. In the below code we will

1. combine the three tables into a single dataframe
2. Nest the results for each athlete and event
3. Join the nested data back into the df
4. Map a function to nested data to filter out dates after the current date

The finished df from this code chunk added all previous results for each swimmer at each date, which then can be used to generate predictions.

```{r dplyr}
df_dplyr <- left_join(Results, Athlete, by = c("ATHLETE" = "Athlete")) %>% ## Add Sex from Athlete table to results
  left_join(Meet, by = c("MEET" = "Meet")) ## Add Meet Date and Course (pool length) from Meet table

df_dplyr <- df_dplyr %>% 
  select(ATHLETE, EVENT, TIME, Start) %>% ## Only keep grouping vars and data to nest
  nest(TIME, Start, .key = "history") %>% ## Time and meet date are nested as their own dfs inside the "history" column
  right_join(df_dplyr) #%>% ## Join nested data back into full df
  #mutate(history = map2(history, Start, function(hist, date){filter(hist, Start < date)})) Saving for later
  ## Maps filter to history to remove "future" data based on meet date
```

A lot of dataframe operations were performed in that code chunk on a medium sized dataframe.  It is easy to follow thanks to dplyr functions and organized comments, but it takes a few seconds to run. With a much larger dataset, and more complicated calculations, dplyr stops being feasible.  Lets benchmark the performance. microbenchmark will run the previous chunk 100 times, and compute the average run time.  Even at only 100 times the calculations, it takes a few minutes.

```{r dplyr_bench}
dplyr_bench <- microbenchmark({
  df_dplyr <- left_join(Results, Athlete, by = c("ATHLETE" = "Athlete")) %>% ## Add Sex from Athlete table to results
    left_join(Meet, by = c("MEET" = "Meet")) ## Add Meet Date and Course (pool length) from Meet table

  df_dplyr <- df_dplyr %>% 
    select(ATHLETE, EVENT, TIME, Start) %>% ## Only keep grouping vars and data to nest
    nest(TIME, Start, .key = "history") %>% ## Time and meet date are nested as their own dfs inside the "history" column
    right_join(df_dplyr, by = c("ATHLETE", "EVENT")) #%>% ## Join nested data back into full df
    #mutate(history = map2(history, Start, function(hist, date){filter(hist, Start < date)})) 
    ## Maps filter to history to remove "future" data based on meet date
})

summary(dplyr_bench) %>% 
  select(-expr)
```

## data.table

Fortunately we have a solution! The data.table package offers a more efficient alternative than the tibbles favored by tidyr and dplyr. We can write much more efficient code, but we lose the intuitive understanding and easy commenting that we have with dplyr.

*Note:* I don't actually know data.table yet. The below code is the "source output from dtplyr". It needs some adjustments to run as data.table.

```{r data.table, eval = FALSE}
`_DT6`[`_DT5`[`_DT4`, on = .(Athlete = ATHLETE)], .(Athlete, 
    EVENT, TIME, Start), on = .(Meet = MEET)][, .(history = list(list(TIME, 
    Start))), keyby = .(Athlete, EVENT)][`_DT6`[`_DT5`[`_DT4`, 
    on = .(Athlete = ATHLETE)], on = .(Meet = MEET)], on = .(EVENT, 
    Athlete)]

```



## dtplyr

The dtplyr package was updated just this August as a viable alternative. It creates function wrappers to apply dplyr functions to data.tables. For more information on the update see this blog post from August 14th:

https://www.r-bloggers.com/big-data-wrangling-4-6m-rows-with-dtplyr-the-new-data-table-backend-for-dplyr/



```{r dtplyr_init}
Results_dt <- lazy_dt(Results)
Athlete_dt <- lazy_dt(Athlete)
Meet_dt <- lazy_dt(Meet)
```

The core mechanic of dtplyr is the use of lazy_dt(). Any dplyr function applied to the result won't actually run code. It will generate efficient data.table code and provide a preview of the final output. The data wrangling is not actually done until you use as.data.table() on the final result.



```{r dtplyr}
df_dtplyr <- left_join(Results_dt, Athlete_dt, by = c("ATHLETE" = "Athlete")) %>% ## Add Sex from Athlete table to results
    left_join(Meet_dt, by = c("MEET" = "Meet")) ## Add Meet Date and Course (pool length) from Meet table
  
df_dtplyr <- df_dtplyr %>% 
    select(Athlete, EVENT, TIME, Start) %>% ## Only keep grouping vars and data to nest
    group_by(Athlete, EVENT) %>% 
    summarize(history = list(list(TIME, Start))) %>% 
    ungroup() %>% 
    right_join(df_dtplyr) %>% 
    as.data.table()
```


```{r dt_bench}
dt_bench <- microbenchmark({
    df_dtplyr <- left_join(Results_dt, Athlete_dt, by = c("ATHLETE" = "Athlete")) %>% ## Add Sex from Athlete table to results
        left_join(Meet_dt, by = c("MEET" = "Meet")) ## Add Meet Date and Course (pool length) from Meet table
      
    df_dtplyr <- df_dtplyr %>% 
        select(Athlete, EVENT, TIME, Start) %>% ## Only keep grouping vars and data to nest
        group_by(Athlete, EVENT) %>% 
        summarize(history = list(list(TIME, Start))) %>% 
        ungroup() %>% 
        right_join(df_dtplyr) %>% 
        as.data.table()
})

summary(dt_bench) %>% select(-expr)

tibble(method = c("dplyr", "dtplyr"), mean = c(summary(dplyr_bench)$mean, summary(dt_bench)$mean))
```

Benchmark summary outputs are in milliseconds. dtplyr was nearly 4 times faster than dplyr


## Weaknesses

dtplyr still has some weaknesses.  The major mechanic that makes it work is that it nests all of the dplyr pipes, and efficiently runs them all at the same time via the data.table package. In a for loop or map function, R will still need to process each computation independently, negating the efficiency of dtplyr.

## Parallel Processing

In that case, we can still speed up the process using parallel processing. There is one step left in transforming the data. For each nested dataframe (or list in dtplyr), we need to filter out anything that happened after the "current" date. We can do this in dplyr with a map function:


```{r}
map_time <- microbenchmark({df_dplyr %>% 
  mutate(history = map2(history, Start, function(hist, date){filter(hist, Start < date)})) 
    ## Maps filter to history to remove "future" data based on meet date
}, times = 5)

summary(map_time) %>% select(-expr)
```

Or with a for loop (using foreach instead of standard for):

```{r parallel}
library(doParallel)
```

```{r foreach}
for_time <- microbenchmark({
  history <- df_dtplyr$history
  foreach(i = 1:length(history)) %do% {
    index <- df_dtplyr$history[[i]][[2]] < df_dtplyr$Start[i] ## T/F of each date less than current
    history[[i]][[1]] <- df_dtplyr$history[[i]][[1]][index]  ## Overwrite Times with filtered by index
    history[[i]][[2]] <- df_dtplyr$history[[i]][[2]][index]  ## Overwrite dates with filtered by index
    } 
}, times = 5)

summary(for_time) %>% select(-expr)
#history[[50]]
```


```{r forparallel}
registerDoParallel(cores = detectCores()-1) ## set number of cores for parallel processing
history <- df_dtplyr$history
par_time <- microbenchmark({
  foreach(i = 1:length(history)) %dopar% {
    index <- df_dtplyr$history[[i]][[2]] < df_dtplyr$Start[i] ## T/F of each date less than current
    history[[i]][[1]] <- df_dtplyr$history[[i]][[1]][index]  ## Overwrite Times with filtered by index
    history[[i]][[2]] <- df_dtplyr$history[[i]][[2]][index]  ## Overwrite dates with filtered by index
    } 
}, times = 5)

summary(par_time) %>% select(-expr)
#history[[50]]

```


On my laptop, this barely performs better than the regular loop, and performs worse than the map2() function from purrr. Without using parallel processing, my laptop is typically around 70% cpu usage when running a code chunk. By using additional clusters, I only gain about 30% speed, and lose efficiency because the cores have to communicate.  

However, my home pc has 6 cores, more RAM, and more processing power. When running a code chunk, my pc is typically around 20% cpu usage. Parallel processing can gain up to 80% computing power, and computing time shows significant improvement.

